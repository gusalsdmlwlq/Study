{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow-serving에서 사용할 모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# savedmodel로 저장\n",
    "# tensorflow-serving에서 사용할 모델 저장할 경로\n",
    "export_path = \"./model/BiLSTM/1\"\n",
    "with tf.Graph().as_default():\n",
    "    sess = tf.Session()\n",
    "#     tf.Saver로 저장한 모델을 불러올 경로\n",
    "    save_path = \"./model/BiLSTM\"\n",
    "    saver = tf.train.import_meta_graph(save_path+\".meta\")\n",
    "    saver.restore(sess, save_path)\n",
    "    graph = tf.get_default_graph()\n",
    "    input_X = graph.get_tensor_by_name(\"BiLSTM/input_X:0\")\n",
    "    dropout_keep_prob = graph.get_tensor_by_name(\"BiLSTM/dropout_keep_prob:0\")\n",
    "    seq_len = graph.get_tensor_by_name(\"BiLSTM/seq_len:0\")\n",
    "    output = graph.get_tensor_by_name(\"BiLSTM/output:0\")\n",
    "#     tensorflow-serving에서 사용할 모델 형식으로 저장\n",
    "    tf.saved_model.simple_save(sess, export_path, \n",
    "                               inputs={\n",
    "                                   \"input_X\": input_X,\n",
    "                                   \"dropout_keep_prob\": dropout_keep_prob,\n",
    "                                   \"seq_len\": seq_len\n",
    "                               }, \n",
    "                               outputs={\n",
    "                                   \"output\": output\n",
    "                               })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Docker로 tensorflow-serving에 모델 배포\n",
    "\n",
    "```\n",
    "docker run -t --rm -p 8501:8501 -v “/E/jhm/open-tube/server/tensorflow/model/BiLSTM:/models/BiLSTM\" -e MODEL_NAME=BiLSTM tensorflow/serving\n",
    "```\n",
    "* 로컬의 ~model/BiLSTM/1에 모델이 저장된 상태\n",
    "* 로컬의 ~model/BiLSTM 경로를 Docker 내부의 /models/BiLSTM 경로에 마운트\n",
    "* Docker 내부의 환경 변수 MODEL_NAME을 모델 이름으로 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow-serving에 접근"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from konlpy.tag import Okt\n",
    "import sys\n",
    "import os\n",
    "import pycurl\n",
    "import re\n",
    "from io import BytesIO\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "okt=Okt()\n",
    "word2vec = Word2Vec.load(\"./../model/word2vec/word2vec.model\")\n",
    "\n",
    "max_length = 100\n",
    "input_size = 300\n",
    "\n",
    "def embedding(word):\n",
    "    if word in word2vec.wv.vocab:\n",
    "        return word2vec.wv[word]\n",
    "    else:\n",
    "        return np.random.normal(size=input_size)\n",
    "\n",
    "X = [\n",
    "    \"아 귀여워 ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ\",\n",
    "    \"아이유 개졸귀 ㅜㅜ너무 귀여워\"\n",
    "]\n",
    "\n",
    "batch_size = len(X)\n",
    "\n",
    "# pycurl을 사용해 tensorflow-serving에 전달\n",
    "c = pycurl.Curl()\n",
    "c.setopt(c.URL, \"http://101.101.164.175:32356/v1/models/BiLSTM:predict\")\n",
    "c.setopt(pycurl.HTTPHEADER, ['Accept: application/json'])\n",
    "\n",
    "token_X = [[\"/\".join(tag) for tag in okt.pos(sentence, norm=True, stem=True)] for sentence in X]\n",
    "batch_X = [[embedding(word) for word in sentence] for sentence in token_X]\n",
    "batch_X_padded = np.zeros(shape=(batch_size, max_length, input_size))\n",
    "for b in range(batch_size):\n",
    "    batch_X_padded[b, :len(batch_X[b])] = batch_X[b]\n",
    "seq_len_ = [len(x) for x in X]\n",
    "\n",
    "# input을 json 형태로 만듬\n",
    "data = json.dumps({\n",
    "    \"inputs\": {\n",
    "        \"input_X\": batch_X_padded.tolist(),\n",
    "        \"dropout_keep_prob\": 1.0,\n",
    "        \"seq_len\": seq_len_\n",
    "    }\n",
    "})\n",
    "\n",
    "buffer = BytesIO()\n",
    "\n",
    "c.setopt(c.POST, True)\n",
    "c.setopt(c.POSTFIELDS, data)\n",
    "c.setopt(c.WRITEFUNCTION, buffer.write)\n",
    "\n",
    "c.perform()\n",
    "\n",
    "# tensorflow serving에 POST로 요청을 보내고 output을 받음\n",
    "body = buffer.getvalue()\n",
    "pred = json.loads(body.decode('utf8'))[\"outputs\"]\n",
    "pred = np.squeeze(pred)\n",
    "print(pred)\n",
    "\n",
    "df = pd.DataFrame(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kubernetes로 tensorflow serving 배포\n",
    "## tensorflow_serving.yaml\n",
    "```\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: jhm-tensorflow-serving\n",
    "  labels:\n",
    "    service-name: jhm-tensorflow-serving\n",
    "spec:\n",
    "  replicas: 1\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      service-name: jhm-tensorflow-serving\n",
    "  template:\n",
    "    metadata:\n",
    "      name: jhm-tensorflow-serving\n",
    "      labels:\n",
    "        service-name: jhm-tensorflow-serving\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: jhm-tensorflow-serving\n",
    "        image: tensorflow/serving\n",
    "        env:\n",
    "        - name: MODEL_NAME\n",
    "          value: \"half_plus_two\"\n",
    "        ports:\n",
    "        - containerPort: 8501\n",
    "        volumeMounts:\n",
    "        - name: volumepath\n",
    "          mountPath: /models/half_plus_two/\n",
    "      volumes:\n",
    "      - name: volumepath\n",
    "        hostPath:\n",
    "          path: /root/serving/tensorflow_serving/servables/tensorflow/testdata/saved_model_half_plus_two_cpu/\n",
    "          type: Directory\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  labels:\n",
    "    run: jhm-tensorflow-serving\n",
    "  name: jhm-tensorflow-serving\n",
    "spec:\n",
    "  ports:\n",
    "  - port: 8501\n",
    "    targetPort: 8501\n",
    "  selector:\n",
    "    service-name: jhm-tensorflow-serving\n",
    "  type: NodePort\n",
    "```\n",
    "```\n",
    "kubectl create -f tensorflow.yaml\n",
    "```\n",
    "\n",
    "* tensorflow-serving 테스트 모델 배포\n",
    "* 터미널에서 curl 명령어로 테스트 모델에 접근\n",
    "```\n",
    "curl -d '{\"instances\": [1.0, 2.0, 5.0]}' -X POST http://localhost:8501/v1/models/half_plus_two:predict \n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'{\\n    \"predictions\": [2.5, 3.0, 4.5\\n    ]\\n}'\n"
     ]
    }
   ],
   "source": [
    "import pycurl\n",
    "from io import BytesIO\n",
    "import json\n",
    "\n",
    "c = pycurl.Curl()\n",
    "c.setopt(c.URL, \"http://101.101.167.71:30857/v1/models/half_plus_two:predict\")\n",
    "c.setopt(pycurl.HTTPHEADER, ['Accept: application/json'])\n",
    "\n",
    "data = json.dumps({\n",
    "    \"instances\": [1.0, 2.0, 5.0]\n",
    "})\n",
    "\n",
    "buffer = BytesIO()\n",
    "\n",
    "c.setopt(c.POST, True)\n",
    "c.setopt(c.POSTFIELDS, data)\n",
    "c.setopt(c.WRITEFUNCTION, buffer.write)\n",
    "\n",
    "c.perform()\n",
    "\n",
    "body = buffer.getvalue()\n",
    "print(body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
