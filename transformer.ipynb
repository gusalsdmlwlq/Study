{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Module, Parameter, Linear, Dropout, ModuleList, Embedding\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import sentencepiece\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./data/WMT16\"\n",
    "train_path_en = os.path.join(path, \"train.tok.clean.bpe.32000.en\")\n",
    "train_path_de = os.path.join(path, \"train.tok.clean.bpe.32000.de\")\n",
    "vocab_path = os.path.join(path, \"vocab.bpe.32000\")\n",
    "test_path_en = os.path.join(path, \"newstest2016.tok.bpe.32000.en\")\n",
    "test_path_en = os.path.join(path, \"newstest2016.tok.bpe.32000.de\")\n",
    "\n",
    "SOS_id = 37005\n",
    "EOS_id = 37006\n",
    "UNK_id = 37007\n",
    "PAD_id = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, path_en, path_de, vocab_path, sos_id, eos_id, unk_id):\n",
    "        super(CustomDataset, self).__init__()\n",
    "        \n",
    "        \"\"\"read English data, German data and vocab\"\"\"\n",
    "        with open(path_en, \"r\") as f:\n",
    "            self.data_en = f.read().split(\"\\n\")[:-1]\n",
    "        with open(path_de, \"r\") as f:\n",
    "            self.data_de = f.read().split(\"\\n\")[:-1]\n",
    "        with open(vocab_path, \"r\") as f:\n",
    "            words = f.read().split(\"\\n\")[:-1]\n",
    "            self.vocab = dict()\n",
    "            for idx, word in enumerate(words):\n",
    "                self.vocab[word] = idx+1\n",
    "            self.vocab[\"<sos>\"] = SOS_id\n",
    "            self.vocab[\"<eos>\"] = EOS_id\n",
    "            self.vocab[\"<unk>\"] = UNK_id\n",
    "            self.vocab[\"<pad>\"] = PAD_id\n",
    "        \"\"\"\"\"\"\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data_en)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # read sentences\n",
    "        sentence_en = self.data_en[idx].split(\" \")\n",
    "        sentence_de = self.data_de[idx].split(\" \")\n",
    "        \n",
    "        tokens_en = []\n",
    "        tokens_de = []\n",
    "        \n",
    "        \"\"\"parse sentences to token sequences\"\"\"\n",
    "        tokens_en.append(self.vocab[\"<sos>\"])\n",
    "        for word in sentence_en:\n",
    "            if word in self.vocab.keys():\n",
    "                tokens_en.append(self.vocab[word])\n",
    "            else:\n",
    "                tokens_en.append(self.vocab[\"<unk>\"])\n",
    "        tokens_en.append(self.vocab[\"<eos>\"])\n",
    "        tokens_en = torch.IntTensor(tokens_en)\n",
    "        \n",
    "        tokens_de.append(self.vocab[\"<sos>\"])\n",
    "        for word in sentence_de:\n",
    "            if word in self.vocab.keys():\n",
    "                tokens_de.append(self.vocab[word])\n",
    "            else:\n",
    "                tokens_de.append(self.vocab[\"<unk>\"])\n",
    "        tokens_de.append(self.vocab[\"<eos>\"])\n",
    "        tokens_de = torch.IntTensor(tokens_de)\n",
    "        \n",
    "        return (tokens_en, tokens_de)\n",
    "        \"\"\"\"\"\"\n",
    "\n",
    "def custom_collate(batch):\n",
    "    \"\"\"\n",
    "    custom collate function for data loader\n",
    "    input: batch of (eq)\n",
    "    output: (sequence_en, de_sequnece, en_seq_len, de_seq_len)\n",
    "    \"\"\"\n",
    "    batch_size = len(batch)\n",
    "    max_len_en = max([len(data[0]) for data in batch])\n",
    "    max_len_de = max([len(data[1]) for data in batch])\n",
    "    sequence_en = torch.zeros([batch_size, max_len_en], dtype=torch.int32)\n",
    "    sequence_de = torch.zeros([batch_size, max_len_de], dtype=torch.int32)\n",
    "    \n",
    "    seq_len_en = []\n",
    "    seq_len_de = []\n",
    "    \n",
    "    for idx, data in enumerate(batch):\n",
    "        seq_en, seq_de = data\n",
    "        \n",
    "        seq_len = len(seq_en)\n",
    "        seq_len_en.append(seq_len)\n",
    "        sequence_en[idx][:seq_len] = seq_en\n",
    "        \n",
    "        seq_len = len(seq_de)\n",
    "        seq_len_de.append(seq_len)\n",
    "        sequence_de[idx][:seq_len] = seq_de\n",
    "    \n",
    "    return sequence_en, sequence_de, seq_len_en, seq_len_de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(train_path_en, train_path_de, vocab_path, SOS_id, EOS_id, UNK_id)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=custom_collate, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(Module):\n",
    "    def __init__(self, d_model, num_heads, dropout):\n",
    "        super(MultiheadAttention, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.d_k = int(d_model / num_heads)\n",
    "        if self.d_k * num_heads != d_model:\n",
    "            raise Exception(\"d_model cannot be divided by num_heads.\")\n",
    "        self.num_heads = num_heads\n",
    "            \n",
    "        self.query = Linear(d_model, d_model)\n",
    "        self.key = Linear(d_model, d_model)\n",
    "        self.value = Linear(d_model, d_model)\n",
    "        \n",
    "        self.dropout = Dropout(dropout)\n",
    "        \n",
    "        self.output = Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, query, key, value, future_mask=None, pad_mask=None):\n",
    "        # query, key, value: [batch, time, d_model]\n",
    "        assert len(query.size()) == 3, \"input is not batch\"\n",
    "        query = self.query(query)\n",
    "        key = self.key(key)\n",
    "        value = self.value(value)\n",
    "        \n",
    "        # query, key, value: [batch * num_heads, time, d_k]\n",
    "        query = torch.cat(torch.split(query, self.d_k, dim=2), dim=0)\n",
    "        key = torch.cat(torch.split(key, self.d_k, dim=2), dim=0)\n",
    "        value = torch.cat(torch.split(value, self.d_k, dim=2), dim=0)\n",
    "        \n",
    "        # attention_score: [batch * num_heads, time, time]\n",
    "        attention_score = torch.matmul(query, key.transpose(1,2)) / np.sqrt(self.d_k)\n",
    "        \n",
    "        # if mask is True, fill to -inf\n",
    "        if future_mask is not None:\n",
    "            attention_score = attention_score.masked_fill(mask=future_mask, value=-float(\"inf\"))\n",
    "        if pad_mask is not None:\n",
    "            # reshape pad_mask from [batch, 1, time] to [batch * num_heads, 1, time]\n",
    "            pad_mask = torch.cat([pad_mask]*self.num_heads, dim=0)\n",
    "            attention_score = attention_score.masked_fill(mask=pad_mask, value=-float(\"inf\"))\n",
    "        \n",
    "        # change score to probability\n",
    "        attention_score = F.softmax(attention_score, dim=2)\n",
    "        attention_score = self.dropout(attention_score)\n",
    "        \n",
    "        # probability * value: [batch * num_heads, time, d_k]\n",
    "        output = torch.matmul(attention_score, value)\n",
    "        \n",
    "        # reshape output: [batch, time, d_model]\n",
    "        batch_size = output.size()[0] / self.num_heads\n",
    "        output = torch.cat(torch.split(output, batch_size, dim=0), dim=2)\n",
    "        \n",
    "        # linear projection of output\n",
    "        output = self.output(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(sequence, pad_id):\n",
    "    # sequence: [batch, time]\n",
    "    batch_size = sequence.size()[0]\n",
    "    seq_len = sequence.size()[1]\n",
    "    \n",
    "    \"\"\"\n",
    "    masking future positions\n",
    "    [F T T]\n",
    "    [F F T]\n",
    "    [F F F]\n",
    "    future_mask: [seq_len, seq_len]\n",
    "    \"\"\"\n",
    "    future_mask = torch.BoolTensor(np.triu(np.ones(seq_len), k=1))\n",
    "    \n",
    "    \"\"\"\n",
    "    masking pad tokens in sequence to prevent query from attending to pad tokens in key\n",
    "    pad_batch: [batch, seq_len]\n",
    "    pad_mask: [batch, 1, seq_len]\n",
    "    pad_mask has to be reshaped to [batch * num_heads, 1, seq_len]\n",
    "    \"\"\"\n",
    "    pad_batch = (sequence == pad_id)\n",
    "    pad_mask = pad_batch.view(batch_size, 1, seq_len)\n",
    "    \n",
    "    return future_mask, pad_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(Module):\n",
    "    def __init__(self, d_model, epsilon=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = Parameter(torch.ones(d_model))\n",
    "        self.beta = Parameter(torch.zeros(d_model))\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        # inputs: [batch, time, d_model]\n",
    "        mean = inputs.mean(dim=2, keepdeem=True)\n",
    "        var = inputs.var(dim=2, keepdeem=True)\n",
    "        \n",
    "        return self.gamma * (inputs - mean) / torch.sqrt(var + self.epsilon) + self.beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(Module):\n",
    "    def __init__(self, d_model, d_ff, dropout):\n",
    "        super(FeedForward, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "        \n",
    "        self.dropout = Dropout(dropout)\n",
    "        self.linear1 = Linear(d_model, d_ff)\n",
    "        self.linear2 = Linear(d_ff, d_model)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        # inputs: [batch, time, d_model]\n",
    "        output = self.linear1(inputs)\n",
    "        output = F.relu(output)\n",
    "        output = self.dropout(output)\n",
    "        output = self.linear2(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(Module):\n",
    "    def __init__(self, d_model, max_len, pad_id):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.max_len = max_len\n",
    "        self.pad_id = pad_id\n",
    "        \n",
    "        self.pe = torch.zeros([max_len, d_model])\n",
    "        for pos in range(max_len):\n",
    "            for i in range(0, d_model, 2):\n",
    "                self.pe[pos, i] = np.sin(pos / 10000 ** (i / d_model))\n",
    "                self.pe[pos, i+1] = np.cos(pos / 10000 ** (i / d_model))\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        # inputs: [batch, time]\n",
    "        batch_size = inputs.size()[0]\n",
    "        seq_len = inputs.size()[1]\n",
    "        \n",
    "        # pad_mask: [batch, time, 1]\n",
    "        pad_mask = (inputs == self.pad_id)\n",
    "        pad_mask = pad_mask.view(batch_size, seq_len, 1)\n",
    "        \n",
    "        # pe: [max_len, d_model] => [batch, seq_len, d_model]\n",
    "        pe = torch.stack([self.pe[:seq_len, :]]*4, dim=0)\n",
    "        pe = pe.masked_fill(mask=pad_mask, value=0)\n",
    "        \n",
    "        return pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(Module):\n",
    "    def __init__(self, d_model, d_ff, num_heads, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.self_attention = MultiheadAttention(d_model, num_heads, dropout)\n",
    "        self.norm1 = LayerNorm(d_model)\n",
    "        self.feedforward = FeedForward(d_model, d_ff)\n",
    "        self.norm2 = LayerNorm(d_model)\n",
    "        self.dropout = Dropout(dropout)\n",
    "        \n",
    "    def forward(self, inputs, pad_mask=None):\n",
    "        \"\"\"\n",
    "        inputs: [batch, time, d_model]\n",
    "        pad_mask: [batch, 1, time]\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"sublayer 1: self attention\"\"\"\n",
    "        output = self.sefl_attention(inputs, inputs, inputs, pad_mask=pad_mask)\n",
    "        output = self.dropout(output)\n",
    "        output_ = self.norm1(output + inputs)\n",
    "        \"\"\"\"\"\"\n",
    "        \n",
    "        \"\"\"sublayer 2: feed forward\"\"\"\n",
    "        output = self.feedforward(output_)\n",
    "        output = self.dropout(output)\n",
    "        output = self.norm2(output + output_)\n",
    "        \"\"\"\"\"\"\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(Module):\n",
    "    def __init__(self, d_model, d_ff, num_heads, dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.self_attention = MultiheadAttention(d_model, num_heads, dropout)\n",
    "        self.norm1 = LayerNorm(d_model)\n",
    "        self.cross_attention = MultiheadAttention(d_model, num_heads, dropout)\n",
    "        self.norm2 = LayerNorm(d_model)\n",
    "        self.feedforward = FeedForward(d_model, d_ff)\n",
    "        self.norm3 = LayerNorm(d_model)\n",
    "        self.dropout = Dropout(dropout)\n",
    "        \n",
    "    def forward(self, inputs, encoder_output, future_mask=None, src_pad_mask=None, tgt_pad_mask=None):\n",
    "        \"\"\"\n",
    "        inputs: [batch, time, d_model]\n",
    "        encoder_output: [batch, time, d_model]\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"sublayer 1: self attention\"\"\"\n",
    "        output = self.self_attention(intput, inputs, inputs, future_mask=future_mask, pad_mask=tgt_pad_mask)\n",
    "        output = self.dropout(output)\n",
    "        output_ = self.norm1(output + inputs)\n",
    "        \"\"\"\"\"\"\n",
    "        \n",
    "        \"\"\"sublayer 2: encoder decoder attention\"\"\"\n",
    "        output = self.cross_attention(output_, encoder_output, encoder_output, pad_mask=src_pad_mask)\n",
    "        output = self.dropout(output)\n",
    "        output_ = self.norm2(output + output_)\n",
    "        \"\"\"\"\"\"\n",
    "        \n",
    "        \"\"\"sublayer 3: feed forward\"\"\"\n",
    "        output = self.feedforward(output_)\n",
    "        output = self.dropout(output)\n",
    "        output = self.norm3(output + output_)\n",
    "        \"\"\"\"\"\"\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderStack(Module):\n",
    "    def __init__(self, d_model, d_ff, shared_embedding, num_heads, num_layers, max_len, dropout, pad_id):\n",
    "        super(EncoderStack, self).__init__()\n",
    "        \n",
    "        self.layers = ModuleList([Encoder(d_model, d_ff, num_heads, dropout)] * num_layers)\n",
    "        self.embedding = shared_embedding\n",
    "        self.pe = PositionalEncoding(d_model, max_len, pad_id)\n",
    "        self.dropout = Dropout(dropout)\n",
    "        \n",
    "    def forward(self, inputs, pad_mask=None):\n",
    "        \"\"\"\n",
    "        inputs: [batch, time]\n",
    "        pad_mask: [batch, 1, time]\n",
    "        \"\"\"\n",
    "        embedding = self.embedding(inputs)\n",
    "        pe = self.pe(inputs)\n",
    "        \n",
    "        output = self.dropout(embedding + pe)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            output = layer(output, pad_mask)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderStack(Module):\n",
    "    def __init__(self, d_model, d_ff, shared_embedding, num_heads, num_layers, max_len, dropout, pad_id):\n",
    "        super(DecoderStack, self).__init__()\n",
    "        \n",
    "        self.layers = ModuleList([Decoder(d_model, d_ff, num_heads, dropout)] * num_layers)\n",
    "        self.embedding = shared_embedding\n",
    "        self.pe = PositionalEncoding(d_model, max_len, pad_id)\n",
    "        self.dropout = Dropout(dropout)\n",
    "        \n",
    "    def forward(self, inputs, encoder_output, future_mask=None, src_pad_mask=None, tgt_pad_mask=None):\n",
    "        \"\"\"\n",
    "        inputs: [batch, time]\n",
    "        encoder_output: [batch, time, d_model]\n",
    "        future_mask: [batch, time, time]\n",
    "        src_pad_mask, tgt_pad_mask: [batch, 1, time]\n",
    "        \"\"\"\n",
    "        embedding = self.embedding(inputs)\n",
    "        pe = self.pe(inputs)\n",
    "        \n",
    "        output = self.dropout(embedding + pe)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            output = layer(output, encoder_output, future_mask, src_pad_mask, tgt_pad_mask)\n",
    "        \n",
    "        # output: [batch, time, d_model] => [batch, time, vocab]\n",
    "        output = torch.matmul(output, self.embedding.weight.data)\n",
    "        output = F.softmax(output, dim=2)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(Module):\n",
    "    def __init__(self, d_model, d_ff, vocab_size, num_heads, num_layers, max_len, dropout, pad_id):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.shared_embedding = Embedding(vocab_size, d_model)\n",
    "        self.encoder = EncoderStack(d_model, d_ff, self.shared_embedding, num_heads, num_layers, max_len, dropout, pad_id)\n",
    "        self.decoder = DecoderStack(d_model, d_ff, self.shared_embedding, num_heads, num_layers, max_len, dropout, pad_id)\n",
    "    \n",
    "    def forward(self, inputs, target, future_mask=None, src_pad_mask=None, tgt_pad_mask=None):\n",
    "        \"\"\"\n",
    "        inputs, target: [batch, time]\n",
    "        future_mask: [batch, time, time]\n",
    "        src_pad_mask, tgt_pad_mask: [batch, 1, time]\n",
    "        \"\"\"\n",
    "        encoder_output = self.encoder(inputs, src_pad_mask)\n",
    "        output = self.decoder(target, encoder_output, future_mask, src_pad_mask, tgt_pad_mask)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
