{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.nn import Module, Linear, Dropout, Parameter\n",
    "from torch.optim import Adam\n",
    "\n",
    "\n",
    "class FeedForward(Module):\n",
    "    def __init__(self, d_model=768, d_ff=2048, dropout=0.2):\n",
    "        super(FeedForward, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "        \n",
    "        self.dropout = Dropout(dropout)\n",
    "        self.linear1 = Linear(d_model, d_ff)\n",
    "        self.linear2 = Linear(d_ff, d_model)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        # inputs: [batch, time, d_model]\n",
    "        output = self.linear1(inputs)\n",
    "        output = F.relu(output)\n",
    "        output = self.dropout(output)\n",
    "        output = self.linear2(output)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    \n",
    "class LayerNorm(Module):\n",
    "    def __init__(self, d_model=768, epsilon=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = Parameter(torch.ones(d_model))\n",
    "        self.beta = Parameter(torch.zeros(d_model))\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        # inputs: [batch, time, d_model]\n",
    "        mean = inputs.mean(dim=2, keepdim=True)\n",
    "        var = inputs.var(dim=2, keepdim=True)\n",
    "        \n",
    "        return self.gamma * (inputs - mean) / torch.sqrt(var + self.epsilon) + self.beta\n",
    "    \n",
    "    \n",
    "class MultiheadAttention(Module):\n",
    "    def __init__(self, d_model=768, num_heads=8, dropout=0.2):\n",
    "        super(MultiheadAttention, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.d_k = int(d_model / num_heads)\n",
    "        if self.d_k * num_heads != d_model:\n",
    "            raise Exception(\"d_model cannot be divided by num_heads.\")\n",
    "        self.num_heads = num_heads\n",
    "            \n",
    "        self.query = Linear(d_model, d_model)\n",
    "        self.key = Linear(d_model, d_model)\n",
    "        self.value = Linear(d_model, d_model)\n",
    "        \n",
    "        self.dropout = Dropout(dropout)\n",
    "        \n",
    "        self.output = Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, query, key, value, future_mask=None, pad_mask=None):\n",
    "        # query, key, value: [batch, time, d_model]\n",
    "        assert len(query.size()) == 3, \"input is not batch\"\n",
    "        query = self.query(query)\n",
    "        key = self.key(key)\n",
    "        value = self.value(value)\n",
    "        \n",
    "        # query, key, value: [batch * num_heads, time, d_k]\n",
    "        query = torch.cat(torch.split(query, self.d_k, dim=2), dim=0)\n",
    "        key = torch.cat(torch.split(key, self.d_k, dim=2), dim=0)\n",
    "        value = torch.cat(torch.split(value, self.d_k, dim=2), dim=0)\n",
    "        \n",
    "        # attention_score: [batch * num_heads, time, time]\n",
    "        attention_score = torch.matmul(query, key.transpose(1,2)) / np.sqrt(self.d_k)\n",
    "        \n",
    "        # if mask is True, fill to -inf\n",
    "        if future_mask is not None:\n",
    "            attention_score = attention_score.masked_fill(mask=future_mask, value=-float(\"inf\"))\n",
    "        if pad_mask is not None:\n",
    "            # reshape pad_mask from [batch, 1, time] to [batch * num_heads, 1, time]\n",
    "            pad_mask = torch.cat([pad_mask]*self.num_heads, dim=0)\n",
    "            attention_score = attention_score.masked_fill(mask=pad_mask, value=-float(\"inf\"))\n",
    "        \n",
    "        # change score to probability\n",
    "        attention_score = F.softmax(attention_score, dim=2)\n",
    "        attention_score = self.dropout(attention_score)\n",
    "        \n",
    "        # probability * value: [batch * num_heads, time, d_k]\n",
    "        output = torch.matmul(attention_score, value)\n",
    "        \n",
    "        # reshape output: [batch, time, d_model]\n",
    "        batch_size = int(output.size()[0] / self.num_heads)\n",
    "        output = torch.cat(torch.split(output, batch_size, dim=0), dim=2)\n",
    "        \n",
    "        # linear projection of output\n",
    "        output = self.output(output)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    \n",
    "class PositionalEncoding(Module):\n",
    "    def __init__(self, d_model=768, max_len=150, pad_id=0):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.max_len = max_len\n",
    "        self.pad_id = pad_id\n",
    "        \n",
    "        self.pe = torch.zeros([max_len, d_model])\n",
    "        for pos in range(max_len):\n",
    "            for i in range(0, d_model, 2):\n",
    "                self.pe[pos, i] = np.sin(pos / 10000 ** (i / d_model))\n",
    "                self.pe[pos, i+1] = np.cos(pos / 10000 ** (i / d_model))\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        # inputs: [batch, time]\n",
    "        batch_size = inputs.size()[0]\n",
    "        seq_len = inputs.size()[1]\n",
    "        \n",
    "        # pad_mask: [batch, time, 1]\n",
    "        pad_mask = (inputs == self.pad_id)\n",
    "        pad_mask = pad_mask.view(batch_size, seq_len, 1)\n",
    "        \n",
    "        # pe: [max_len, d_model] => [batch, seq_len, d_model]\n",
    "        pe = torch.stack([self.pe[:seq_len, :]]*batch_size, dim=0)\n",
    "        pe = pe.masked_fill(mask=pad_mask, value=0)\n",
    "        \n",
    "        return pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, d_model=512, d_ff=2048, num_heads=8, dropout=0.2):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        \n",
    "        self.self_attention = MultiheadAttention(d_model, num_heads, dropout)\n",
    "        self.norm1 = LayerNorm(d_model)\n",
    "        self.feedforward = FeedForward(d_model, d_ff, dropout)\n",
    "        self.norm2 = LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, inputs, future_mask=None, tgt_pad_mask=None):\n",
    "        output = self.self_attention(inputs, inputs, inputs, future_mask=future_mask, pad_mask=tgt_pad_mask)\n",
    "        output = self.dropout(output)\n",
    "        output_ = self.norm1(output + inputs)\n",
    "\n",
    "        output = self.feedforward(output_)\n",
    "        output = self.dropout(output)\n",
    "        output = self.norm2(output + output_)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, shared_embedding, d_model=512, d_ff=2048, num_heads=8, num_layers=6, max_len=100, dropout=0.2, pad_id=0):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        \n",
    "        self.linear = nn.Linear(d_model*2, d_model)\n",
    "        layers = []\n",
    "        layer = DecoderBlock(d_model, d_ff, num_heads, dropout)\n",
    "        for i in range(num_layers):\n",
    "            layers.append(layer)\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.embedding = shared_embedding\n",
    "        self.pe = PositionalEncoding(self.embedding.embedding_dim, max_len, pad_id)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, decoder_inputs, encoder_output, future_mask=None, tgt_pad_mask=None):\n",
    "        embedding = self.embedding(decoder_inputs)\n",
    "        pe = self.pe(decoder_inputs)\n",
    "        embedding = embedding + pe\n",
    "        length = decoder_inputs.size(1)\n",
    "        inputs = torch.cat([embedding, encoder_output[:, 0, :].unsqueeze(dim=1).repeat(1, length, 1)], dim=2)\n",
    "        inputs = self.linear(self.dropout(inputs))\n",
    "        output = self.dropout(inputs)\n",
    "        for layer in self.layers:\n",
    "            output = layer(output, future_mask, tgt_pad_mask)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_size = 100\n",
    "hidden_size = 128\n",
    "embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "transformer = TransformerDecoder(embedding, 128, 512, 4, 3)\n",
    "optim = Adam(transformer.parameters(), lr=3e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "encoder_output = torch.rand(4, 10, 128)\n",
    "decoder_inputs = torch.randint(0, 100, size=(4, 10))\n",
    "outputs = transformer(decoder_inputs, encoder_output)\n",
    "loss = outputs.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss.backward()\n",
    "optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(transformer.layers[0].parameters()) == list(transformer.layers[1].parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerDecoder(\n",
       "  (linear): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (layers): ModuleList(\n",
       "    (0): DecoderBlock(\n",
       "      (self_attention): MultiheadAttention(\n",
       "        (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (output): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (feedforward): FeedForward(\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "      )\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "    (1): DecoderBlock(\n",
       "      (self_attention): MultiheadAttention(\n",
       "        (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (output): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (feedforward): FeedForward(\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "      )\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "    (2): DecoderBlock(\n",
       "      (self_attention): MultiheadAttention(\n",
       "        (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (output): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (feedforward): FeedForward(\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "      )\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (embedding): Embedding(100, 128)\n",
       "  (pe): PositionalEncoding()\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
