{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from gensim.models import Word2Vec\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from konlpy.tag import Okt\n",
    "import sys\n",
    "import os\n",
    "np.random.seed(1)\n",
    "tf.set_random_seed(1)\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2,3\"\n",
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"./data/ratings_train.txt\",encoding=\"UTF-8\")\n",
    "train_data = [line.split(\"\\t\")[1:] for line in f.read().splitlines()]\n",
    "train_data = train_data[1:]\n",
    "f = open(\"./data/ratings_test.txt\",encoding=\"UTF-8\")\n",
    "test_data = [line.split(\"\\t\")[1:] for line in f.read().splitlines()]\n",
    "test_data = test_data[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150000\n",
      "['아 더빙.. 진짜 짜증나네요 목소리', '0']\n",
      "50000\n",
      "['굳 ㅋ', '1']\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data))\n",
    "print(train_data[0])\n",
    "print(len(test_data))\n",
    "print(test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149995\n",
      "49997\n"
     ]
    }
   ],
   "source": [
    "for idx, data in enumerate(train_data):\n",
    "    if data[0] == \"\":\n",
    "        del train_data[idx]\n",
    "for idx, data in enumerate(test_data):\n",
    "    if data[0] == \"\":\n",
    "        del test_data[idx]\n",
    "print(len(train_data))\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-ab1fb50af1cd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"/\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtag\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mokt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstem\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtest_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"/\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtag\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mokt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstem\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-ab1fb50af1cd>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"/\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtag\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mokt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstem\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtest_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"/\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtag\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mokt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstem\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36-32\\Lib\\conda\\lib\\site-packages\\konlpy\\tag\\_okt.py\u001b[0m in \u001b[0;36mpos\u001b[1;34m(self, phrase, norm, stem, join)\u001b[0m\n\u001b[0;32m     61\u001b[0m                     \u001b[0mphrase\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m                     \u001b[0mjpype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBoolean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                     jpype.java.lang.Boolean(stem)).toArray()\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mjoin\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36-32\\Lib\\conda\\lib\\site-packages\\jpype\\_jclass.py\u001b[0m in \u001b[0;36m_getClassFor\u001b[1;34m(javaClass)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m \u001b[1;32mdef\u001b[0m \u001b[0m_getClassFor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjavaClass\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m     \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjavaClass\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetName\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_CLASSES\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_tokens = [[\"/\".join(tag) for tag in okt.pos(sentence[0], norm=True, stem=True)] for sentence in train_data]\n",
    "test_tokens = [[\"/\".join(tag) for tag in okt.pos(sentence[0], norm=True, stem=True)] for sentence in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec = Word2Vec(size=300, alpha=0.025, seed=1, sg=1, min_alpha=0.025)\n",
    "# word2vec.build_vocab(tokens)\n",
    "# for epoch in range(30):\n",
    "#     word2vec.train(train_tokens, total_examples=word2vec.corpus_count, epochs=word2vec.iter)\n",
    "#     word2vec.alpha -= 0.002\n",
    "#     word2vec.min_alpha = word2vec.alpha\n",
    "#     print(\"epochs: {}/30\".format(epoch+1))\n",
    "# word2vec.save(\"./model/word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jeon\\AppData\\Local\\Programs\\Python\\Python36-32\\Lib\\conda\\lib\\site-packages\\smart_open\\smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "word2vec = Word2Vec.load(\"./model/word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding(word):\n",
    "    if word in word2vec.wv.vocab:\n",
    "        return word2vec.wv[word]\n",
    "    else:\n",
    "        return np.random.normal(size=(300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_X = [[embedding(word) for word in sentence] for sentence in train_tokens]\n",
    "# train_y = [[int(data[1])] for data in train_data]\n",
    "# test_X = [[embedding(word) for word in sentence] for sentence in test_tokens]\n",
    "# test_y = [[int(data[1])] for data in test_data]\n",
    "# print(len(train_X))\n",
    "# print(len(train_y))\n",
    "# print(len(test_X))\n",
    "# print(len(test_y))\n",
    "# np.save(\"./data/train_X.npy\",train_X)\n",
    "# np.save(\"./data/train_y.npy\",train_y)\n",
    "# np.save(\"./data/test_X.npy\",test_X)\n",
    "# np.save(\"./data/test_y.npy\",test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = np.load(\"./data/train_X.npy\")\n",
    "train_y = np.load(\"./data/train_y.npy\")\n",
    "test_X = np.load(\"./data/test_X.npy\")\n",
    "test_y = np.load(\"./data/test_y.npy\")\n",
    "print(len(train_X))\n",
    "print(len(train_y))\n",
    "print(len(test_X))\n",
    "print(len(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq_length = [len(x) for x in train_X]\n",
    "test_seq_length = [len(x) for x in test_X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM:\n",
    "    def __init__(self, input_size, layers, hidden_units, max_length, learning_rate, dropout_keep_prob):\n",
    "        with tf.variable_scope(\"BiLSTM\", reuse=tf.AUTO_REUSE):\n",
    "            self.input_X = tf.placeholder(dtype=tf.float32, shape=[None, max_length, input_size], name=\"input_X\")\n",
    "            self.input_y = tf.placeholder(dtype=tf.float32, shape=[None,1], name=\"input_y\")\n",
    "            self.dropout_keep_prob = tf.placeholder(dtype=tf.float32, shape=(), name=\"dropout_keep_prob\")\n",
    "            self.seq_len = tf.placeholder(dtype=tf.int32, shape=[None], name=\"seq_len\")\n",
    "            \n",
    "            self.output = self.bilstm(self.input_X, layers, hidden_units, self.dropout_keep_prob)\n",
    "            \n",
    "            self.loss = -tf.reduce_mean(self.input_y*tf.log(self.output) + (1-self.input_y)*tf.log(1-self.output))\n",
    "            self.train = tf.train.AdamOptimizer(learning_rate).minimize(self.loss)\n",
    "            self.pred = tf.equal(self.input_y, tf.cast(tf.to_int32(self.output >= 0.5),tf.float32))\n",
    "            self.acc = tf.multiply(tf.reduce_mean(tf.cast(self.pred, tf.float32)), 100, name=\"acc\")\n",
    "            \n",
    "            tf.summary.scalar(name=\"Loss\", tensor=self.loss)\n",
    "            tf.summary.scalar(name=\"Accuracy\", tensor=self.acc)\n",
    "            self.merge_graph = tf.summary.merge_all()\n",
    "    \n",
    "    def bilstm(self, X, layers, hidden_units, dropout_keep_prob):\n",
    "        \n",
    "        fw_cell_stack = []\n",
    "        for i in range(layers):\n",
    "            fw_cell =  tf.nn.rnn_cell.LSTMCell(hidden_units, forget_bias=1.0, state_is_tuple=True)\n",
    "            fw_cell = tf.nn.rnn_cell.DropoutWrapper(fw_cell, output_keep_prob=dropout_keep_prob)\n",
    "            fw_cell_stack.append(fw_cell)\n",
    "        fw_lstm_cell = tf.nn.rnn_cell.MultiRNNCell(fw_cell_stack)\n",
    "        \n",
    "        bw_cell_stack = []\n",
    "        for i in range(layers):\n",
    "            bw_cell =  tf.nn.rnn_cell.LSTMCell(hidden_units,forget_bias=1.0, state_is_tuple=True)\n",
    "            bw_cell = tf.nn.rnn_cell.DropoutWrapper(bw_cell, output_keep_prob=dropout_keep_prob)\n",
    "            bw_cell_stack.append(bw_cell)\n",
    "        bw_lstm_cell = tf.nn.rnn_cell.MultiRNNCell(bw_cell_stack)\n",
    "        \n",
    "#         fw_lstm_cell =  tf.nn.rnn_cell.LSTMCell(hidden_units)\n",
    "#         fw_lstm_cell = tf.nn.rnn_cell.DropoutWrapper(fw_lstm_cell, output_keep_prob=dropout_keep_prob)\n",
    "#         bw_lstm_cell =  tf.nn.rnn_cell.LSTMCell(hidden_units)\n",
    "#         bw_lstm_cell = tf.nn.rnn_cell.DropoutWrapper(bw_lstm_cell, output_keep_prob=dropout_keep_prob)\n",
    "        \n",
    "        outputs,states = tf.nn.bidirectional_dynamic_rnn(cell_fw=fw_lstm_cell, cell_bw=bw_lstm_cell, inputs=X, dtype=tf.float32, sequence_length=self.seq_len)\n",
    "        fw_output = outputs[0][:,-1]\n",
    "        bw_output = outputs[1][:,0]\n",
    "        \n",
    "        W = tf.get_variable(\"W_output\",shape=[2*hidden_units,1], initializer=tf.contrib.layers.xavier_initializer(), dtype=tf.float32)\n",
    "        b = tf.get_variable(\"b_output\",shape=(), initializer=tf.zeros_initializer(), dtype=tf.float32)\n",
    "#         output = tf.concat([states[0][1], states[1][1]], axis=1)\n",
    "        output = tf.concat([fw_output, bw_output], axis=1)\n",
    "        output = tf.nn.sigmoid(tf.matmul(output,W) + b, name=\"output\")\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 300\n",
    "layers = 1\n",
    "hidden_units = 128\n",
    "batch_size = 32\n",
    "dropout_keep_prob = 0.75\n",
    "max_length = 100\n",
    "learning_rate = 0.001\n",
    "epochs = 2\n",
    "n_class = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session started\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'BiLSTM' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-b70ef96fb97c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0msess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Session started\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBiLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_units\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout_keep_prob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Model was initialized\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0msave_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"./model/BiLSTM\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'BiLSTM' is not defined"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    start = time.time()\n",
    "    \n",
    "    sess = tf.Session()\n",
    "    print(\"Session started\")\n",
    "    model = BiLSTM(input_size, layers, hidden_units, max_length, learning_rate, dropout_keep_prob)\n",
    "    print(\"Model was initialized\")\n",
    "    save_path = \"./model/BiLSTM\"\n",
    "    n_batchs = (int)(len(train_X) / batch_size)\n",
    "    saver = tf.train.Saver()\n",
    "    writer = tf.summary.FileWriter(\"./log\", sess.graph)\n",
    "    merge_graph = model.merge_graph\n",
    "    global_steps = 0\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    def train_step(batch_X, batch_y, seq_lenght, steps, epoch):\n",
    "        \n",
    "        batch_X_padded = np.zeros(shape=(batch_size,max_length,input_size))\n",
    "        for b in range(batch_size):\n",
    "            batch_X_padded[b,:len(batch_X[b])] = batch_X[b]\n",
    "        \n",
    "        feed_dict = {\n",
    "            model.input_X: batch_X_padded,\n",
    "            model.input_y: batch_y,\n",
    "            model.dropout_keep_prob: dropout_keep_prob,\n",
    "            model.seq_len: seq_length\n",
    "        }\n",
    "        _, loss, acc = sess.run([model.train, model.loss, model.acc], feed_dict=feed_dict)\n",
    "        if steps % 100 == 0:\n",
    "            print(\"epochs: {}/{}, iters: {}, loss: {}, acc: {}\".format(epoch+1,epochs,steps, loss, acc))\n",
    "            summary = sess.run(merge_graph, feed_dict=feed_dict)\n",
    "            writer.add_summary(summary, global_steps)\n",
    "    for i in range(epochs):\n",
    "        steps = 0\n",
    "        for j in range(n_batchs):\n",
    "            batch_X, batch_y = train_X[steps*batch_size:(steps+1)*batch_size], train_y[steps*batch_size:(steps+1)*batch_size]\n",
    "            seq_length = train_seq_length[steps*batch_size:(steps+1)*batch_size]\n",
    "            try:\n",
    "                train_step(batch_X, batch_y, seq_length, steps, i)\n",
    "            except ValueError:\n",
    "                print(ValueError)\n",
    "            except KeyboardInterrupt:\n",
    "                saver.save(sess,save_path)\n",
    "                print(\"Model was saved\")\n",
    "                sys.exit(1)\n",
    "            steps += 1\n",
    "            global_steps += 1\n",
    "        saver.save(sess,save_path)\n",
    "    saver.save(sess,save_path)\n",
    "    print(\"Training finished\")\n",
    "    print(\"Model was saved\")\n",
    "    \n",
    "    end = time.time()\n",
    "    print(\"{} secs\".format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-234073a1b0c5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest_len\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10000\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mbatchs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_len\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_X' is not defined"
     ]
    }
   ],
   "source": [
    "test_len = len(test_X)\n",
    "\n",
    "batch_size = 10000\n",
    "batchs = int(test_len/batch_size)\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    sess = tf.Session()\n",
    "    print(\"Session started\")\n",
    "    save_path = \"./model/BiLSTM\"\n",
    "    saver = tf.train.import_meta_graph(save_path+\".meta\")\n",
    "    saver.restore(sess, save_path)\n",
    "    graph = tf.get_default_graph()\n",
    "    print(\"Model was loaded\")\n",
    "    \n",
    "    input_X = graph.get_tensor_by_name(\"BiLSTM/input_X:0\")\n",
    "    input_y = graph.get_tensor_by_name(\"BiLSTM/input_y:0\")\n",
    "    dropout_keep_prob = graph.get_tensor_by_name(\"BiLSTM/dropout_keep_prob:0\")\n",
    "    output = graph.get_tensor_by_name(\"BiLSTM/output:0\")\n",
    "    seq_len = graph.get_tensor_by_name(\"BiLSTM/seq_len:0\")\n",
    "    acc = graph.get_tensor_by_name(\"BiLSTM/acc:0\")\n",
    "    \n",
    "    steps = 0\n",
    "    acc_total = 0\n",
    "    for b in range(batchs):\n",
    "        batch_X, batch_y = test_X[steps*batch_size:(steps+1)*batch_size], test_y[steps*batch_size:(steps+1)*batch_size]\n",
    "        batch_X_padded = np.zeros(shape=(batch_size,max_length,input_size))\n",
    "        for b in range(batch_size):\n",
    "            batch_X_padded[b,:len(batch_X[b])] = batch_X[b]\n",
    "        seq_len_ = test_seq_length[steps*batch_size:(steps+1)*batch_size]\n",
    "        feed_dict = {\n",
    "            input_X: batch_X_padded,\n",
    "            input_y: batch_y,\n",
    "            dropout_keep_prob: 1.0,\n",
    "            seq_len: seq_len_\n",
    "        }\n",
    "        acc_batch = sess.run(acc, feed_dict=feed_dict)\n",
    "        acc_total += acc_batch\n",
    "        steps += 1\n",
    "        print(\"Batch {} 정확도: {}\".format(steps,acc_batch))\n",
    "    batch_X, batch_y = test_X[steps*batch_size:], test_y[steps*batch_size:]\n",
    "    batch_size = len(batch_X)\n",
    "    print(\"남은 데이터 수: {}\".format(batch_size))\n",
    "    batch_X_padded = np.zeros(shape=(batch_size,max_length,input_size))\n",
    "    for b in range(batch_size):\n",
    "        batch_X_padded[b,:len(batch_X[b])] = batch_X[b]\n",
    "    seq_len_ = test_seq_length[steps*batch_size:(steps+1)*batch_size]\n",
    "    feed_dict = {\n",
    "        input_X: batch_X_padded,\n",
    "        input_y: batch_y,\n",
    "        dropout_keep_prob: 1.0,\n",
    "        seq_len: seq_len_\n",
    "    }\n",
    "    acc_batch = sess.run(acc, feed_dict=feed_dict)\n",
    "    acc_total += acc_batch\n",
    "    steps += 1\n",
    "    print(\"Batch {} 정확도: {}\".format(steps,acc_batch))\n",
    "    print(\"전체 정확도: {}\".format(acc_total/(steps)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "리뷰 \"우와 영화 진짜 재밌다.\" 은 약 98%의 확률로 긍정 리뷰입니다.\n",
      "리뷰 \"연기 개못하네 ㅡㅡ\" 은 약 4%의 확률로 긍정 리뷰입니다.\n",
      "리뷰 \"명품 연기.. 역시\" 은 약 96%의 확률로 긍정 리뷰입니다.\n",
      "리뷰 \"돈 아깝다\" 은 약 0%의 확률로 긍정 리뷰입니다.\n",
      "리뷰 \"이런걸 영화라고 만듬..?\" 은 약 8%의 확률로 긍정 리뷰입니다.\n",
      "리뷰를 입력하세요.(exit => 종료)영화 대박이네\n",
      "리뷰 \"영화 대박이네\" 은 약 83%의 확률로 긍정 리뷰입니다.\n",
      "리뷰를 입력하세요.(exit => 종료)영화 대박이네\n",
      "리뷰 \"영화 대박이네\" 은 약 82%의 확률로 긍정 리뷰입니다.\n",
      "리뷰를 입력하세요.(exit => 종료)영화 대박이네\n",
      "리뷰 \"영화 대박이네\" 은 약 82%의 확률로 긍정 리뷰입니다.\n",
      "리뷰를 입력하세요.(exit => 종료)영화 대박이네\n",
      "리뷰 \"영화 대박이네\" 은 약 80%의 확률로 긍정 리뷰입니다.\n",
      "리뷰를 입력하세요.(exit => 종료)영화 대박이네\n",
      "리뷰 \"영화 대박이네\" 은 약 82%의 확률로 긍정 리뷰입니다.\n",
      "리뷰를 입력하세요.(exit => 종료)exit\n"
     ]
    }
   ],
   "source": [
    "batch_size = 5\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    sess = tf.Session()\n",
    "    save_path = \"./model/BiLSTM\"\n",
    "    saver = tf.train.import_meta_graph(save_path+\".meta\")\n",
    "    saver.restore(sess, save_path)\n",
    "    graph = tf.get_default_graph()\n",
    "    \n",
    "    input_X = graph.get_tensor_by_name(\"BiLSTM/input_X:0\")\n",
    "    input_y = graph.get_tensor_by_name(\"BiLSTM/input_y:0\")\n",
    "    dropout_keep_prob = graph.get_tensor_by_name(\"BiLSTM/dropout_keep_prob:0\")\n",
    "    output = graph.get_tensor_by_name(\"BiLSTM/output:0\")\n",
    "    seq_len = graph.get_tensor_by_name(\"BiLSTM/seq_len:0\")\n",
    "    acc = graph.get_tensor_by_name(\"BiLSTM/acc:0\")\n",
    "    \n",
    "    X = [\n",
    "        \"우와 영화 진짜 재밌다.\",\n",
    "        \"연기 개못하네 ㅡㅡ\",\n",
    "        \"명품 연기.. 역시\",\n",
    "        \"돈 아깝다\",\n",
    "        \"이런걸 영화라고 만듬..?\"\n",
    "    ]\n",
    "    token_X = [[\"/\".join(tag) for tag in okt.pos(sentence, norm=True, stem=True)] for sentence in X]\n",
    "    batch_X = [[embedding(word) for word in sentence] for sentence in token_X]\n",
    "    batch_X_padded = np.zeros(shape=(batch_size,max_length,input_size))\n",
    "    for b in range(batch_size):\n",
    "        batch_X_padded[b,:len(batch_X[b])] = batch_X[b]\n",
    "    seq_len_ = [len(x) for x in batch_X]\n",
    "    feed_dict = {\n",
    "        input_X: batch_X_padded,\n",
    "        dropout_keep_prob: 1.0,\n",
    "        seq_len: seq_len_\n",
    "    }\n",
    "    pred = sess.run(output, feed_dict=feed_dict)\n",
    "    for idx, p in enumerate(pred):\n",
    "        print(\"리뷰 \\\"{}\\\" 은 약 {}%의 확률로 긍정 리뷰입니다.\".format(X[idx], int(p[0]*100)))\n",
    "    batch_size = 1\n",
    "    while True:\n",
    "        X = input(\"리뷰를 입력하세요.(exit => 종료)\")\n",
    "        if X == \"exit\":\n",
    "            break\n",
    "        X = [X]\n",
    "        token_X = [[\"/\".join(tag) for tag in okt.pos(sentence, norm=True, stem=True)] for sentence in X]\n",
    "        batch_X = [[embedding(word) for word in sentence] for sentence in token_X]\n",
    "        batch_X_padded = np.zeros(shape=(batch_size, max_length, input_size))\n",
    "        for b in range(batch_size):\n",
    "            batch_X_padded[b, :len(batch_X[b])] = batch_X[b]\n",
    "        seq_len_ = [len(x) for x in X]\n",
    "        feed_dict = {\n",
    "            input_X: batch_X_padded,\n",
    "            dropout_keep_prob: 0.0,\n",
    "            seq_len: seq_len_\n",
    "        }\n",
    "        pred = sess.run(output, feed_dict=feed_dict)\n",
    "        print(\"리뷰 \\\"{}\\\" 은 약 {}%의 확률로 긍정 리뷰입니다.\".format(X[0], int(pred[0]*100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# savedmodel로 저장\n",
    "# tensorflow-serving에서 사용할 모델 저장할 경로\n",
    "export_path = \"./model/BiLSTM/1\"\n",
    "with tf.Graph().as_default():\n",
    "    sess = tf.Session()\n",
    "#     tf.Saver로 저장한 모델을 불러올 경로\n",
    "    save_path = \"./model/BiLSTM\"\n",
    "    saver = tf.train.import_meta_graph(save_path+\".meta\")\n",
    "    saver.restore(sess, save_path)\n",
    "    graph = tf.get_default_graph()\n",
    "    input_X = graph.get_tensor_by_name(\"BiLSTM/input_X:0\")\n",
    "    dropout_keep_prob = graph.get_tensor_by_name(\"BiLSTM/dropout_keep_prob:0\")\n",
    "    seq_len = graph.get_tensor_by_name(\"BiLSTM/seq_len:0\")\n",
    "    output = graph.get_tensor_by_name(\"BiLSTM/output:0\")\n",
    "#     tensorflow-serving에서 사용할 모델 형식으로 저장\n",
    "    tf.saved_model.simple_save(sess, export_path, \n",
    "                               inputs={\n",
    "                                   \"input_X\": input_X,\n",
    "                                   \"dropout_keep_prob\": dropout_keep_prob,\n",
    "                                   \"seq_len\": seq_len\n",
    "                               }, \n",
    "                               outputs={\n",
    "                                   \"output\": output\n",
    "                               })\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
